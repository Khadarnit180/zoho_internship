{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a43c9663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/local/ZOHOCORP/abdul-pt6532/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(2000, 784)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data_hound = pd.read_csv(\"hound.txt\", delimiter='\\t', header=None)\n",
    "data_hound.columns = [\"Review_text\", \"Review_class\"]\n",
    "data_sample = pd.read_csv(\"sample.txt\", delimiter='\\t', header=None)\n",
    "data_sample.columns = [\"Review_text\", \"Review_class\"]\n",
    "data = pd.concat([data_hound,data_sample])\n",
    "data_hound.loc[1,[\"Review_text\"]]\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "def clean_text(df):\n",
    "    all_reviews = list()\n",
    "    lines = df[\"Review_text\"].values.tolist()\n",
    "    for text in lines:\n",
    "        text = text.lower()\n",
    "        pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        text = pattern.sub('', text)\n",
    "        text = re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "        tokens = word_tokenize(text)\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_words.discard(\"not\")\n",
    "        PS = PorterStemmer()\n",
    "#         words = [w for w in words if not w in stop_words]\n",
    "        words = [PS.stem(w) for w in words if not w in stop_words]\n",
    "        words = ' '.join(words)\n",
    "        all_reviews.append(words)\n",
    "    return all_reviews\n",
    "\n",
    "all_reviews = clean_text(data)\n",
    "stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "CV = CountVectorizer(min_df=3)   \n",
    "X = CV.fit_transform(all_reviews).toarray()\n",
    "y = data.loc[:,[\"Review_class\"]]\n",
    "y=y.dropna()\n",
    "y=y.to_numpy()\n",
    "print(type(X),type(y))\n",
    "print(X.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d39eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c338d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = X_train.T\n",
    "y_train = y_train.reshape(1, y_train.shape[0])\n",
    "X_test = X_test.T\n",
    "y_test = y_test.reshape(1, y_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbe4aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_structure(X, Y):\n",
    "    input_unit = X.shape[0] # size of input layer\n",
    "    hidden_unit = 4 #hidden layer of size 4\n",
    "    output_unit = Y.shape[0] # size of output layer\n",
    "    return (input_unit, hidden_unit, output_unit)\n",
    "(input_unit, hidden_unit, output_unit) = define_structure(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd7106b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters_initialization(input_unit, hidden_unit, output_unit):\n",
    "    #np.random.seed(2) \n",
    "    W1 = np.random.randn(hidden_unit, input_unit)*0.01\n",
    "    b1 = np.zeros((hidden_unit, 1))\n",
    "    W2 = np.random.randn(output_unit, hidden_unit)*0.01\n",
    "    b2 = np.zeros((output_unit, 1))\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b9890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    cache = {\"Z1\": Z1,\"A1\": A1,\"Z2\": Z2,\"A2\": A2}\n",
    "    \n",
    "    return A2, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31ee7a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_cost(A2, Y, parameters):\n",
    "    m = Y.shape[1] \n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1-Y), np.log(1 - A2))\n",
    "    cost = - np.sum(logprobs)/ m\n",
    "    cost = float(np.squeeze(cost))                       \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c98d4b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    #number of training example\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "\n",
    "    dZ2 = A2-Y\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T) \n",
    "    db1 = (1/m)*np.sum(dZ1, axis=1,keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2,\"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2815e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(parameters, grads, learning_rate = 0.001):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "   \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\"W1\": W1, \"b1\": b1,\"W2\": W2,\"b2\": b2}\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7f8430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(X, Y, hidden_unit, num_iterations = 1000):\n",
    "    np.random.seed(3)\n",
    "    input_unit = define_structure(X, Y)[0]\n",
    "    output_unit = define_structure(X, Y)[2]\n",
    "    \n",
    "    parameters = parameters_initialization(input_unit, hidden_unit, output_unit)\n",
    "   \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        cost = cross_entropy_cost(A2, Y, parameters)\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = gradient_descent(parameters, grads)\n",
    "\n",
    "    return parameters\n",
    "parameters = neural_network_model(X_train, y_train, 4, num_iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cba43db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Train: 51%\n"
     ]
    }
   ],
   "source": [
    "def prediction(parameters, X):\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = np.round(A2)\n",
    "    \n",
    "    return predictions\n",
    "predictions = prediction(parameters, X_train)\n",
    "print ('Accuracy Train: %d' % float((np.dot(y_train, predictions.T) + np.dot(1 - y_train, 1 - predictions.T))/float(y_train.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db14e6f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
