{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "755f68b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representation of document1:  [[1 0 1 1 1 1 1 1 1 1 1 0]]\n",
      "Representation of document2:  [[0 1 1 0 0 0 0 1 0 1 1 1]]\n",
      "\n",
      "los is  tensor(0.5780, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.5570, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.5368, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.5173, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.4985, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.4804, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.4629, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.4461, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.4299, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.4143, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.3992, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.3847, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.3707, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.3573, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.3443, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.3318, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.3197, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.3081, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.2969, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.2861, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.2757, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.2657, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.2561, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.2468, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.2378, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.2292, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.2208, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.2128, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.2051, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.1976, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.1904, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.1835, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.1769, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.1704, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.1642, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.1583, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.1525, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.1470, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.1416, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.1365, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.1315, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.1268, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.1222, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.1177, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.1134, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.1093, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.1053, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.1015, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0978, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0943, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0908, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0875, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0844, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0813, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0783, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0755, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0728, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0701, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0676, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0651, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0627, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0605, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0583, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0562, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0541, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0521, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0503, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0484, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0467, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0450, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0433, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0418, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0402, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0388, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0374, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0360, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0347, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0334, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0322, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0311, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0299, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0288, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0278, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0268, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0258, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0249, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0240, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0231, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0223, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0215, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0207, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0199, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0192, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0185, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0178, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0172, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0166, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0160, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0154, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0148, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0143, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0138, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0133, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0128, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0123, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0119, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0114, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0110, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0106, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0102, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0099, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0095, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0092, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0088, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0085, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0082, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0079, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0073, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0071, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0068, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0066, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0063, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0061, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "los is  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1.]])\n",
      "tensor([[ 0.0547,  0.9158,  0.9608, -0.0058,  0.0139, -0.0362,  0.0332,  0.9752,\n",
      "          0.0076,  0.9909,  0.9130,  0.9268]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "document1 = 'Dog hates a cat It loves to go out and play'\n",
    "document2 = 'Cat loves to play with a ball'\n",
    "\n",
    "# converting sentences to lower case\n",
    "document1 = document1.lower()\n",
    "document2 = document2.lower()\n",
    "\n",
    "# Intialize BoWs\n",
    "count_vect = CountVectorizer()\n",
    "# fit the corpus to CountVectorizer\n",
    "count_vect.fit([document1, document2])\n",
    "\n",
    "#--------print(\"feature names \", count_vect.get_feature_names())\n",
    "\n",
    "# bag of word representation of document1\n",
    "bow1 = count_vect.transform([document1])\n",
    "t1=bow1.toarray()\n",
    "print(\"Representation of document1: \", t1)\n",
    "\n",
    "# bag of word representation of document2\n",
    "bow2 = count_vect.transform([document2])\n",
    "t2=bow2.toarray()\n",
    "print(\"Representation of document2: \", t2)\n",
    "\n",
    "# Output:\n",
    "# feature names  ['and', 'ball', 'cat', 'dog', 'go', 'hates', 'it', 'loves', 'out', 'play', 'to', 'with']\n",
    "\n",
    "# Representation of document1:  [[1 0 1 1 1 1 1 1 1 1 1 0]]\n",
    "# Representation of document2:  [[0 1 1 0 0 0 0 1 0 1 1 1]]\n",
    "\n",
    "\n",
    "t2.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn import MSELoss, Parameter\n",
    "import torch.nn as nn\n",
    "\n",
    "x_train = torch.tensor(t1, dtype=torch.float)\n",
    "y_train = torch.tensor(t2, dtype=torch.float)\n",
    "model = nn.Linear(12,12)\n",
    "criterion = MSELoss()\n",
    "# optimizer = torch.optim.SGD(model, lr=0.1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "#model = nn.Linear(1,1)\n",
    "for epoch in range(150):#500\n",
    "    # Remove the grad computed in the last step\n",
    "    #optimizer.zero_grad()\n",
    "    # Run a + bx\n",
    "    y_predicted = model(x_train)\n",
    "\n",
    "    \n",
    "    \n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    loss.backward()\n",
    "    print()\n",
    "    print(\"los is \",loss)\n",
    "  \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "print(y_train)   \n",
    "print(f\"{y_predicted}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0983d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f41977e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 4, #features: 1\n",
      "Prediction before training: f(5) = -1.099\n",
      "epoch  1 : w =  -0.025721346959471703  loss =  tensor(34.2729, grad_fn=<MseLossBackward0>)\n",
      "epoch  11 : w =  1.3224525451660156  loss =  tensor(1.1378, grad_fn=<MseLossBackward0>)\n",
      "epoch  21 : w =  1.5496941804885864  loss =  tensor(0.2659, grad_fn=<MseLossBackward0>)\n",
      "epoch  31 : w =  1.5963279008865356  loss =  tensor(0.2296, grad_fn=<MseLossBackward0>)\n",
      "epoch  41 : w =  1.6136133670806885  loss =  tensor(0.2157, grad_fn=<MseLossBackward0>)\n",
      "epoch  51 : w =  1.6258891820907593  loss =  tensor(0.2031, grad_fn=<MseLossBackward0>)\n",
      "epoch  61 : w =  1.6370786428451538  loss =  tensor(0.1913, grad_fn=<MseLossBackward0>)\n",
      "epoch  71 : w =  1.647821307182312  loss =  tensor(0.1801, grad_fn=<MseLossBackward0>)\n",
      "epoch  81 : w =  1.658227801322937  loss =  tensor(0.1697, grad_fn=<MseLossBackward0>)\n",
      "epoch  91 : w =  1.6683239936828613  loss =  tensor(0.1598, grad_fn=<MseLossBackward0>)\n",
      "Prediction after training: f(5) = 9.335\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x \n",
    "\n",
    "# here : f = 2 * x\n",
    "\n",
    "# # 0) Training samples, watch the shape!\n",
    "# X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "# Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "x_train = torch.tensor(t1, dtype=torch.float)\n",
    "y_train = torch.tensor(t2, dtype=torch.float)\n",
    "n_samples, n_features = X.shape\n",
    "print(f'#samples: {n_samples}, #features: {n_features}')\n",
    "# 0) create a test sample\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "# 1) Design Model, the model has to implement the forward pass!\n",
    "# Here we can use a built-in model from PyTorch\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# we can call this model with samples X\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "'''\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define diferent layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "model = LinearRegression(input_size, output_size)\n",
    "'''\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# 2) Define loss and optimizer\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass with our model\n",
    "    y_predicted = model(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #new---------------------utube\n",
    "        [w, b] = model.parameters() # unpack parameters\n",
    "        print('epoch ', epoch+1, ': w = ', w[0][0].item(), ' loss = ', l)\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "110a81b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representation   [[1 0 1 1 1 1 1 1 1 1 1 0]]\n",
      "Representation   [[0 1 1 0 0 0 0 1 0 1 1 1]]\n",
      "torch.Size([1, 12])\n",
      "#samples: 1, #features: 12\n",
      "hi\n",
      "epoch  1  loss =  tensor(0.5352, grad_fn=<MseLossBackward0>)\n",
      "hi\n",
      "epoch  11  loss =  tensor(0.3697, grad_fn=<MseLossBackward0>)\n",
      "hi\n",
      "epoch  21  loss =  tensor(0.2553, grad_fn=<MseLossBackward0>)\n",
      "hi\n",
      "epoch  31  loss =  tensor(0.1763, grad_fn=<MseLossBackward0>)\n",
      "hi\n",
      "epoch  41  loss =  tensor(0.1218, grad_fn=<MseLossBackward0>)\n",
      "hi\n",
      "epoch  51  loss =  tensor(0.0841, grad_fn=<MseLossBackward0>)\n",
      "hi\n",
      "epoch  61  loss =  tensor(0.0581, grad_fn=<MseLossBackward0>)\n",
      "hi\n",
      "epoch  71  loss =  tensor(0.0401, grad_fn=<MseLossBackward0>)\n",
      "hi\n",
      "epoch  81  loss =  tensor(0.0277, grad_fn=<MseLossBackward0>)\n",
      "hi\n",
      "epoch  91  loss =  tensor(0.0191, grad_fn=<MseLossBackward0>)\n",
      "weight Parameter containing:\n",
      "tensor([[-0.0720, -0.1248,  0.0178,  0.2380, -0.2203, -0.1335,  0.2373, -0.0935,\n",
      "         -0.0609,  0.0253,  0.2082, -0.1067],\n",
      "        [ 0.3031,  0.1274, -0.0449,  0.2301,  0.0970,  0.1153,  0.0522,  0.1575,\n",
      "         -0.0114, -0.1716,  0.1199,  0.1571],\n",
      "        [ 0.2994, -0.2125, -0.2281,  0.0423,  0.1655,  0.2963,  0.1442,  0.2291,\n",
      "          0.0441,  0.3107, -0.2532, -0.1502],\n",
      "        [-0.1864, -0.0469, -0.0977, -0.2389,  0.0776,  0.0882, -0.2145,  0.2066,\n",
      "          0.0454,  0.0604,  0.1345, -0.1920],\n",
      "        [ 0.0942,  0.1408, -0.0540, -0.1972,  0.0108,  0.1552,  0.1866, -0.2782,\n",
      "          0.0986,  0.0740,  0.0328, -0.0957],\n",
      "        [ 0.2317,  0.1309,  0.1426, -0.1587,  0.1215,  0.2343, -0.1246, -0.0608,\n",
      "         -0.2302, -0.1380, -0.1831, -0.0614],\n",
      "        [-0.2442,  0.0442, -0.1692,  0.3063, -0.2037, -0.2225,  0.2991,  0.2531,\n",
      "          0.0467,  0.1518, -0.1470,  0.1964],\n",
      "        [ 0.0372,  0.1366,  0.2575,  0.2898, -0.1419,  0.1116,  0.1040,  0.1023,\n",
      "         -0.2285,  0.0843,  0.0678, -0.1004],\n",
      "        [-0.0151, -0.1448,  0.1374, -0.2197,  0.1474,  0.1662, -0.2120, -0.2231,\n",
      "          0.0182,  0.1767,  0.0857, -0.0401],\n",
      "        [ 0.0771,  0.0734,  0.0135, -0.1443,  0.3028, -0.0090,  0.1050, -0.1267,\n",
      "          0.3598,  0.1223, -0.1653, -0.1985],\n",
      "        [ 0.1339,  0.2705,  0.1954,  0.1797,  0.1624,  0.0567, -0.1340,  0.0931,\n",
      "          0.1177, -0.1477,  0.0404, -0.2734],\n",
      "        [-0.1352, -0.0503, -0.0484,  0.1920,  0.3516,  0.2666,  0.0333,  0.3067,\n",
      "         -0.1282, -0.1210,  0.0051,  0.1303]], requires_grad=True)\n",
      "bias Parameter containing:\n",
      "tensor([-0.1571,  0.0925, -0.0976,  0.0806, -0.1331,  0.2376, -0.1190,  0.2092,\n",
      "         0.0276,  0.2999,  0.2410, -0.0272], requires_grad=True)\n",
      "\n",
      "\n",
      "Prediction after training: tensor([[-0.0108,  0.9398,  0.9528, -0.0442, -0.0105,  0.0723, -0.0487,  0.8933,\n",
      "          0.0892,  0.8351,  0.9387,  0.6954]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_328677/4119524788.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# 12*1  1*12 =1*1 ///////////////////////////////////////////////\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "document1 = 'Dog hates a cat It loves to go out and play'\n",
    "document2 = 'Cat loves to play with a ball'\n",
    "\n",
    "# converting sentences to lower case\n",
    "document1 = document1.lower()\n",
    "document2 = document2.lower()\n",
    "\n",
    "# Intialize BoWs\n",
    "count_vect = CountVectorizer()\n",
    "# fit the corpus to CountVectorizer\n",
    "count_vect.fit([document1, document2])\n",
    "\n",
    "#--------print(\"feature names \", count_vect.get_feature_names())\n",
    "\n",
    "# bag of word representation of document1\n",
    "bow1 = count_vect.transform([document1])\n",
    "t1=bow1.toarray()\n",
    "print(\"Representation  \", t1)\n",
    "\n",
    "# bag of word representation of document2\n",
    "bow2 = count_vect.transform([document2])\n",
    "t2=bow2.toarray()\n",
    "print(\"Representation  \", t2)\n",
    "\n",
    "# Output:\n",
    "# feature names  ['and', 'ball', 'cat', 'dog', 'go', 'hates', 'it', 'loves', 'out', 'play', 'to', 'with']\n",
    "\n",
    "# Representation of document1:  [[1 0 1 1 1 1 1 1 1 1 1 0]]\n",
    "# Representation of document2:  [[0 1 1 0 0 0 0 1 0 1 1 1]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x \n",
    "\n",
    "# here : f = 2 * x\n",
    "\n",
    "# X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "# Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "X = torch.tensor(t1, dtype=torch.float32)\n",
    "Y = torch.tensor(t2, dtype=torch.float32)\n",
    "print(Y.shape)\n",
    "n_samples, n_features = X.shape\n",
    "print(f'#samples: {n_samples}, #features: {n_features}')\n",
    "# 0) create a test sample\n",
    "X_test = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "# 1) Design Model, the model has to implement the forward pass!\n",
    "# Here we can use a built-in model from PyTorch\n",
    "\n",
    "\n",
    "# we can call this mod2el with samples X\n",
    "model = nn.Linear(12,12)\n",
    "\n",
    "\n",
    "# 2) Define loss and optimizer\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass with our model\n",
    "    y_predicted = model(X_test)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"hi\")\n",
    "        print('epoch ', epoch+1,' loss = ', l)\n",
    "for i,j in model.named_parameters():\n",
    "    print(i,j)\n",
    "print()\n",
    "print()\n",
    "print(f\"Prediction after training: {model(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa2a6f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
