{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f0af1bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 1 1 1 1 1 1 1 1 0]]\n",
      "[[0 1 1 0 0 0 0 1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "document1 = 'Dog hates a cat It loves to go out and play'\n",
    "document2 = 'Cat loves to play with a ball'\n",
    "\n",
    "# converting sentences to lower case\n",
    "document1 = document1.lower()\n",
    "document2 = document2.lower()\n",
    "\n",
    "# Intialize BoWs\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "# fit the corpus to CountVectorizer\n",
    "count_vect.fit([document1, document2])\n",
    "\n",
    "#-print(\"feature names \", count_vect.get_feature_names())\n",
    "\n",
    "# bag of word representation of document1\n",
    "bow1 = count_vect.transform([document1])\n",
    "t1=bow1.toarray()\n",
    "print(t1)\n",
    "\n",
    "# bag of word representation of document2\n",
    "bow2 = count_vect.transform([document2])\n",
    "t2=bow2.toarray()\n",
    "print( t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25596eb2",
   "metadata": {},
   "source": [
    "#TRAINING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f9a6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "model parameterrs for best accuracy ------------------------\n",
      "<generator object Module.named_parameters at 0x7fc3948a2ff0>\n",
      "Parameter containing:\n",
      "tensor([[ 0.0458],\n",
      "        [ 0.1724],\n",
      "        [ 0.1432],\n",
      "        [-0.0563],\n",
      "        [-0.0877],\n",
      "        [-0.1679],\n",
      "        [-0.0926],\n",
      "        [ 0.2831],\n",
      "        [-0.1511],\n",
      "        [ 0.3255],\n",
      "        [ 0.1383],\n",
      "        [ 0.3398]], requires_grad=True) Parameter containing:\n",
      "tensor([-0.1962,  0.0777,  0.2654,  0.3127,  0.5040,  0.8013,  0.4667, -0.4754,\n",
      "         0.7866, -0.6253,  0.2591, -0.6972], requires_grad=True)\n",
      "prediction before trainig : tensor([[0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1.]])\n",
      "hyper parameters- for best accuracy --------------------------------\n",
      "learning rate 0.01 number of epochs: 99\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from matplotlib import pyplot\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "X_test = torch.tensor(t1, dtype=torch.float32)\n",
    "Y = torch.tensor(t2, dtype=torch.float32)\n",
    "n_samples, n_features = X_test.shape\n",
    "\n",
    "# 0) create a test sample\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "model = nn.Linear(n_samples,n_features)\n",
    "\n",
    "# 2) Define loss and optimizer\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass with our model\n",
    "    y_predicted = model(X_test)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "   # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "#     if epoch % 10 == 0:\n",
    "print('loss = ', l )    \n",
    "print()\n",
    "print(\"model parameterrs for best accuracy ------------------------\")\n",
    "l= model.named_parameters()\n",
    "print(l)\n",
    "# for i,j in model.named_parameters():\n",
    "#             print(i,j )\n",
    "# print()\n",
    "\n",
    "print(model.weight,model.bias)\n",
    "print(\"prediction before trainig :\",Y)\n",
    "print(\"hyper parameters- for best accuracy --------------------------------\")\n",
    "print(\"learning rate\",learning_rate,    \"number of epochs:\",epoch)\n",
    "# print(f\"Prediction after training: {model(X_test).item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b26e91",
   "metadata": {},
   "source": [
    "# after keeping learning rate to half and observing prediction .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b03fdc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss =  tensor(2.0505e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "model parameterrs for best accuracy ------------------------\n",
      "\n",
      "hyper parameters- for best accuracy --------------------------------\n",
      "learning rate 0.005 number of epochs: 99\n",
      "Prediction after training: tensor([ 3.6680e-03,  9.9327e-01,  9.9794e-01,  3.5175e-03,  7.3407e-03,\n",
      "        -4.2624e-03,  4.0108e-04,  9.9329e-01,  3.4754e-03,  1.0003e+00,\n",
      "         9.9445e-01,  1.0002e+00], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "learning_rate = 0.01/2\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "\n",
    "#3) Training loop\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass with our model\n",
    "    y_predicted = model(X_test)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "print(' loss = ', l)\n",
    "print()\n",
    "print(\"model parameterrs for best accuracy ------------------------\")\n",
    "\n",
    "print()\n",
    "print(\"hyper parameters- for best accuracy --------------------------------\")\n",
    "print(\"learning rate\",learning_rate,    \"number of epochs:\",epoch)\n",
    "print(f\"Prediction after training: {model(X_test)}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2e954c",
   "metadata": {},
   "source": [
    "# observation :accuracy improved for small learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fe9081f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Review_text  Review_class\n",
      "0  So there is no way for me to plug it in here i...             0\n",
      "1                        Good case, Excellent value.             1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/local/ZOHOCORP/abdul-pt6532/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "     Review_class\n",
      "0               0\n",
      "1               1\n",
      "2               1\n",
      "3               0\n",
      "4               1\n",
      "..            ...\n",
      "995             0\n",
      "996             0\n",
      "997             0\n",
      "998             0\n",
      "999             0\n",
      "\n",
      "[2000 rows x 1 columns]\n",
      "torch.Size([2000, 784])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import pandas as pd \n",
    "data_hound = pd.read_csv(\"hound.txt\", delimiter='\\t', header=None)\n",
    "data_hound.columns = [\"Review_text\", \"Review_class\"]\n",
    "\n",
    "data_sample = pd.read_csv(\"sample.txt\", delimiter='\\t', header=None)\n",
    "data_sample.columns = [\"Review_text\", \"Review_class\"]\n",
    "\n",
    "# data_yelp = pd.read_csv(\"yelp_labelled.txt\", delimiter='\\t', header=None)\n",
    "# data_yelp.columns = [\"Review_text\", \"Review_class\"]\n",
    "\n",
    "data = pd.concat([data_hound,data_sample])\n",
    "data_hound.loc[1,[\"Review_text\"]]\n",
    "print(data[:2])\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(df):\n",
    "    all_reviews = list()\n",
    "    lines = df[\"Review_text\"].values.tolist()\n",
    "    for text in lines:\n",
    "        text = text.lower()\n",
    "        pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        text = pattern.sub('', text)\n",
    "        text = re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "        tokens = word_tokenize(text)\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_words.discard(\"not\")\n",
    "        PS = PorterStemmer()\n",
    "#         words = [w for w in words if not w in stop_words]\n",
    "        words = [PS.stem(w) for w in words if not w in stop_words]\n",
    "        words = ' '.join(words)\n",
    "        all_reviews.append(words)\n",
    "    return all_reviews\n",
    "\n",
    "all_reviews = clean_text(data)\n",
    "#all_reviews[0:20]\n",
    "stopwords.words(\"english\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "CV = CountVectorizer(min_df=3)   \n",
    "X = CV.fit_transform(all_reviews).toarray()\n",
    "y = data.loc[:,[\"Review_class\"]]\n",
    "print(type(X))\n",
    "print(type(y))\n",
    "y=y.dropna()\n",
    "print(X[1])\n",
    "print(y)\n",
    "\n",
    "X=torch.tensor(X,dtype=torch.float32)\n",
    "y=y.to_numpy()#we cant chge directly from data frame  to tensor so 1st np array  then  torch\n",
    "y=torch.tensor(y,dtype=torch.float32)\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4da306ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1600, 784])\n",
      "befr t tensor([0.0197], grad_fn=<AddBackward0>)\n",
      "loss =  tensor(0.4076, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "model parameterrs for best accuracy ------------------------\n",
      "hyper parameters- for best accuracy --------------------------------\n",
      "learning rate 0.001 number of epochs: 99\n",
      "tensor([[0.1404]], grad_fn=<AddmmBackward0>)\n",
      "h tensor([[0.]], grad_fn=<RoundBackward0>)\n",
      "tensor([[1.]])\n",
      "accuracy: 0.5475\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "# X = torch.tensor(X, dtype=torch.float32)\n",
    "# Y = torch.tensor(y, dtype=torch.float32)------------1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# print(X_train)\n",
    "n_samples, n_features = X_train.shape\n",
    "print(X_train.shape)\n",
    "model = nn.Linear(n_features,1)\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_iters = 100\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# print(\"i\",X_train[1])\n",
    "y_predicted = model(X_train[1])\n",
    "print(\"befr t\",y_predicted)\n",
    "# 3) Training loop\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass with our model\n",
    "    y_predicted = model(X_train)\n",
    "\n",
    "    # loss\n",
    "    l = loss(y_train, y_predicted)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "   # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "#     if epoch % 10 == 0:\n",
    "print('loss = ', l )    \n",
    "print()\n",
    "print(\"model parameterrs for best accuracy ------------------------\")\n",
    "# l= model.named_parameters()\n",
    "# print(l)\n",
    "# for i,j in model.named_parameters():\n",
    "#             print(i,j )\n",
    "#print(model.weight,model.bias)\n",
    "\n",
    "print(\"hyper parameters- for best accuracy --------------------------------\")\n",
    "print(\"learning rate\",learning_rate,    \"number of epochs:\",epoch)\n",
    "output=model(X_test[:1,])\n",
    "print(output)\n",
    "\n",
    "y_predicted_cls = output.round()\n",
    "print(\"h\",y_predicted_cls)\n",
    "print(y_test[:1])\n",
    "acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "print(f'accuracy: {acc.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98480f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 784])\n",
      "784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/ZOHOCORP/abdul-pt6532/miniconda3/envs/py_torch/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([2000, 10])) that is different to the input size (torch.Size([2000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  tensor(0.4093, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "model parameterrs for best accuracy ------------------------\n",
      "<generator object Module.named_parameters at 0x7fc38c8784a0>\n",
      "hyper parameters- for best accuracy --------------------------------\n",
      "learning rate 0.01 number of epochs: 99\n",
      "tensor([[0.1083, 0.0876, 0.0916, 0.1058, 0.1087, 0.1036, 0.0972, 0.0957, 0.0900,\n",
      "         0.1116]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# X = torch.tensor(, dtype=torch.float32)\n",
    "# Y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "print(X.shape)\n",
    "n_samples, n_features = X.shape\n",
    "print(n_features)\n",
    "# 0) create a test sample-----\n",
    "# print(X)\n",
    "'''\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,):\n",
    "        \n",
    "         super().__init__()\n",
    "         self.hidden = nn.Linear(n_features, 64)\n",
    "         self.output = nn.Linear(64, 1)\n",
    "         self.sigmoid = nn.Sigmoid()\n",
    "         self.softmax = nn.Softmax(dim=0)\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = Model()'''\n",
    "\n",
    "\n",
    "\n",
    "input_size = 784\n",
    "hidden_sizes =64\n",
    "output_size = 10\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes, output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "# print(model)\n",
    "# model = nn.Sequential(nn.Linear(n_features,256),nn.Linear(256,1))\n",
    "\n",
    "# 2) Define loss and optimizer\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass with our model\n",
    "    y_predicted = model(X)\n",
    "    #print(y_predicted.shape)\n",
    "    # loss\n",
    "    l = loss(y, y_predicted)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "   # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "print('loss = ', l )    \n",
    "print()\n",
    "print(\"model parameterrs for best accuracy ------------------------\")\n",
    "l= model.named_parameters()\n",
    "print(l)\n",
    "# for i,j in model.named_parameters():\n",
    "#             print(i,j )\n",
    "# print(model.weight,model.bias)\n",
    "\n",
    "print(\"hyper parameters- for best accuracy --------------------------------\")\n",
    "print(\"learning rate\",learning_rate,    \"number of epochs:\",epoch)\n",
    "\n",
    "\n",
    "output=model(X[:1])\n",
    "print(output)\n",
    "\n",
    "# y_predicted_cls = output.round()\n",
    "# acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "# print(f'accuracy: {acc.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4856039a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  tensor(0.4092, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "model parameterrs for best accuracy ------------------------\n",
      "\n",
      "hyper parameters- for best accuracy --------------------------------\n",
      "learning rate 0.005 number of epochs: 99\n",
      "tensor([[0.0941, 0.1137, 0.0929,  ..., 0.1071, 0.0972, 0.1053],\n",
      "        [0.0927, 0.1109, 0.0913,  ..., 0.1057, 0.1011, 0.1068],\n",
      "        [0.0944, 0.1121, 0.0915,  ..., 0.1057, 0.1020, 0.1031],\n",
      "        ...,\n",
      "        [0.0924, 0.1109, 0.0889,  ..., 0.1081, 0.1018, 0.1055],\n",
      "        [0.0927, 0.1125, 0.0917,  ..., 0.1058, 0.1013, 0.1010],\n",
      "        [0.0941, 0.1117, 0.0904,  ..., 0.1055, 0.1015, 0.1034]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "learning_rate/=2\n",
    "n_iters = 100\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass with our model\n",
    "    y_predicted = model(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(y, y_predicted)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "   # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "#     if epoch % 10 == 0:\n",
    "print('loss = ', l )    \n",
    "print()\n",
    "print(\"model parameterrs for best accuracy ------------------------\")\n",
    "# l= model.named_parameters()\n",
    "# print(l)\n",
    "# for i,j in model.named_parameters():\n",
    "#             print(i,j )\n",
    "print()\n",
    "print(\"hyper parameters- for best accuracy --------------------------------\")\n",
    "print(\"learning rate\",learning_rate,\"number of epochs:\",epoch)\n",
    "output=model(X)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa01b053",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
