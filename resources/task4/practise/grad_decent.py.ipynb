{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1c8898e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t_one =torch.Tensor([[1,1,1],[2,2,2],[3,3,3]]) \n",
    "print(t_one)\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "68b8c46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', Parameter containing:\n",
      "tensor([[-0.6081]], requires_grad=True))\n",
      "('bias', Parameter containing:\n",
      "tensor([0.6983], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "model=nn.Linear(1,1)\n",
    "for i in model.named_parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92d0ef5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', Parameter containing:\n",
      "tensor([[0.0696]], requires_grad=True))\n",
      "('bias', Parameter containing:\n",
      "tensor([0.8162], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Linear(1,1)\n",
    "\n",
    "for i in model.named_parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa15e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import sklearn\n",
    "\n",
    "models = [sklearn.linear_model.LinearRegression]\n",
    "for m in models:\n",
    "    hyperparams = inspect.signature(m.__init__)\n",
    "    print(hyperparams)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c691339",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_two =torch.Tensor([[1,1,1],[2,2,2],[3,3,3]]) \n",
    "print(t_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e57fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_prod=torch.mm(t_one,t_two)\n",
    "print(dot_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7c8a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#forword pass\n",
    "w=torch.tensor(1.0,requires_grad=True)\n",
    "x=torch.tensor(1.0)\n",
    "y=torch.tensor(2.0)\n",
    "#forword pass ip op \n",
    "yy=w*x\n",
    "loss=(y-yy)**2\n",
    "#backward pass going back from op and apply grad for ip\n",
    "loss.backward()\n",
    "print(w.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d73a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# #model = nn.Linear(1,1)\n",
    "# for j in model.named_parameters():\n",
    "#     if j.requires_grad:\n",
    "#         #print(j)   \n",
    "#         print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d8e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stochastic .graddescent in pytorch optimizers\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn import MSELoss, Parameter\n",
    "\n",
    "# The data function is: y = x + 10\n",
    "x_train = torch.tensor([1, 2, 3, 4])\n",
    "y_train = torch.tensor([11.0, 12.0, 13.0, 14.0])#, dtype=torch.float\n",
    "\n",
    "# Simple Linear Regression: a + bx\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "\n",
    "# If we use nn.Module to create a model, it will model.parameters()\n",
    "model = [Parameter(a), Parameter(b)]\n",
    "\n",
    "criterion = MSELoss()\n",
    "# optimizer = torch.optim.SGD(model, lr=0.1)\n",
    "optimizer = torch.optim.Adam(model, lr=0.1)\n",
    "\n",
    "for epoch in range(10):#500\n",
    "    # Remove the grad computed in the last step\n",
    "    optimizer.zero_grad()\n",
    "    # Run a + bx\n",
    "    y_predicted = model[0] + model[1] * x_train\n",
    "    print(x_train, y_train, y_predicted)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    loss.backward()\n",
    "    print(\"los is \",loss)\n",
    "    optimizer.step()\n",
    "    #print(model[0].grad)\n",
    "    #rint(model[1].grad)\n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd940d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Linear(1,1)\n",
    "for i,j in model.named_parameters():\n",
    "        if j.requires_grad:\n",
    "            print(j )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef6696f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.8162], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.8162], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "#model = nn.Linear(1,1)\n",
    "for i,j in model.named_parameters():\n",
    "        print(j)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27a839c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', Parameter containing:\n",
      "tensor([[0.0696]], requires_grad=True))\n",
      "('bias', Parameter containing:\n",
      "tensor([0.8162], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for i in model.named_parameters():\n",
    "    print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f3aff186",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [47], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Run a + bx\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#y_predicted = model[0] + model[1] * x_train\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m y_predicted \u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x_train, y_train, y_predicted)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "# SGD: Stochastic Gradient Descent\n",
    "# Useful links\n",
    "# https://ruder.io/optimizing-gradient-descent/\n",
    "# https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e\n",
    "\n",
    "# STEPS\n",
    "# 1. Compute Loss\n",
    "# 2. Compute Partial Derivatives\n",
    "# 3. Update the Parameters\n",
    "# 4. Repeat 1-3 until convergence\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn import MSELoss, Parameter\n",
    "\n",
    "# The data function is: y = x + 10\n",
    "x_train = torch.tensor([1, 2, 3, 4])\n",
    "y_train = torch.tensor([11, 12, 13, 14], dtype=torch.float)\n",
    "\n",
    "# Simple Linear Regression: a + bx\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "\n",
    "# If we use nn.Module to create a model, it will model.parameters()\n",
    "model = [Parameter(a), Parameter(b)]\n",
    "\n",
    "criterion = MSELoss()\n",
    "# optimizer = torch.optim.SGD(model, lr=0.1)\n",
    "optimizer = torch.optim.Adam(model, lr=0.1)\n",
    "\n",
    "for epoch in range(500):\n",
    "    # Remove the grad computed in the last step\n",
    "    optimizer.zero_grad()\n",
    "    # Run a + bx\n",
    "    #y_predicted = model[0] + model[1] * x_train\n",
    "    y_predicted =model(x_train)\n",
    "    if epoch % 10 == 0:\n",
    "        print(x_train, y_train, y_predicted)\n",
    "        loss = criterion(y_predicted, y_train)\n",
    "        loss.backward()\n",
    "        print(loss)\n",
    "    optimizer.step()\n",
    "#     print(model[0].grad)\n",
    "#     print(model[1].grad)\n",
    "  \n",
    "   \n",
    "    # To get the model hyperparameters before you instantiate the class.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492d3c8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import sklearn\n",
    "a = sklearn.ensemble.RandomForestRegressor\n",
    "models = [sklearn.ensemble.RandomForestRegressor, sklearn.linear_model.LinearRegression]\n",
    "\n",
    "for m in models:\n",
    "    hyperparams = \n",
    "    #inspect.getargspec(m.__init__).args\n",
    "    print(hyperparams) # Do something with them here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9680aca5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import sklearn\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "reg = RandomForestClassifier()\n",
    "params = reg.get_params()\n",
    "# do something...\n",
    "reg.set_params(params)\n",
    "reg.fit(X,  y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22580433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import sklearn\n",
    "get_params(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88930623",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To get the model hyperparameters before you instantiate the class\n",
    "import inspect\n",
    "import sklearn\n",
    "\n",
    "models = [sklearn.linear_model.LinearRegression]\n",
    "for m in models:\n",
    "    hyperparams = inspect.signature(m.__init__)\n",
    "    print(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5915bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f771ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "SVR._get_param_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7ac9aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representation   [[1 0 1 1 1 1 1 1 1 1 1 0]]\n",
      "Representation   [[0 1 1 0 0 0 0 1 0 1 1 1]]\n",
      "torch.Size([1, 12])\n",
      "#samples: 1, #features: 12\n",
      "Prediction before training: = tensor([[-0.8725]], grad_fn=<AddmmBackward0>)\n",
      "hi\n",
      "epoch  1  loss =  tensor(2.1336, grad_fn=<MseLossBackward0>)\n",
      "hi\n",
      "epoch  11  loss =  tensor(0.2631, grad_fn=<MseLossBackward0>)\n",
      "hi\n",
      "epoch  21  loss =  tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "hi\n",
      "epoch  31  loss =  tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "hi\n",
      "epoch  41  loss =  tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "hi\n",
      "epoch  51  loss =  tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "hi\n",
      "epoch  61  loss =  tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "hi\n",
      "epoch  71  loss =  tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "hi\n",
      "epoch  81  loss =  tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "hi\n",
      "epoch  91  loss =  tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "weight Parameter containing:\n",
      "tensor([[ 0.2297, -0.1712, -0.1498,  0.0541,  0.1275,  0.2568,  0.0488, -0.0393,\n",
      "         -0.1008,  0.0545,  0.1353, -0.1870]], requires_grad=True)\n",
      "bias Parameter containing:\n",
      "tensor([-0.1168], requires_grad=True)\n",
      "\n",
      "\n",
      "Prediction after training: tensor([[0.5000]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_374211/3846949331.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# 12*1  1*12 =1*1 ///////////////////////////////////////////////\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "document1 = 'Dog hates a cat It loves to go out and play'\n",
    "document2 = 'Cat loves to play with a ball'\n",
    "\n",
    "# converting sentences to lower case\n",
    "document1 = document1.lower()\n",
    "document2 = document2.lower()\n",
    "\n",
    "# Intialize BoWs\n",
    "count_vect = CountVectorizer()\n",
    "# fit the corpus to CountVectorizer\n",
    "count_vect.fit([document1, document2])\n",
    "\n",
    "#--------print(\"feature names \", count_vect.get_feature_names())\n",
    "\n",
    "# bag of word representation of document1\n",
    "bow1 = count_vect.transform([document1])\n",
    "t1=bow1.toarray()\n",
    "print(\"Representation  \", t1)\n",
    "\n",
    "# bag of word representation of document2\n",
    "bow2 = count_vect.transform([document2])\n",
    "t2=bow2.toarray()\n",
    "print(\"Representation  \", t2)\n",
    "\n",
    "# Output:\n",
    "# feature names  ['and', 'ball', 'cat', 'dog', 'go', 'hates', 'it', 'loves', 'out', 'play', 'to', 'with']\n",
    "\n",
    "# Representation of document1:  [[1 0 1 1 1 1 1 1 1 1 1 0]]\n",
    "# Representation of document2:  [[0 1 1 0 0 0 0 1 0 1 1 1]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x \n",
    "\n",
    "# here : f = 2 * x\n",
    "\n",
    "# X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "# Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "X = torch.tensor(t1, dtype=torch.float32)\n",
    "Y = torch.tensor(t2, dtype=torch.float32)\n",
    "print(Y.shape)\n",
    "n_samples, n_features = X.shape\n",
    "print(f'#samples: {n_samples}, #features: {n_features}')\n",
    "# 0) create a test sample\n",
    "X_test = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "# 1) Design Model, the model has to implement the forward pass!\n",
    "# Here we can use a built-in model from PyTorch\n",
    "\n",
    "\n",
    "# we can call this model with samples X\n",
    "model = nn.Linear(12,1)\n",
    "\n",
    "\n",
    "print(f'Prediction before training: = {model(X_test)}')    #practise\n",
    "\n",
    "# 2) Define loss and optimizer\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass with our model\n",
    "    y_predicted = model(X_test)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"hi\")\n",
    "        print('epoch ', epoch+1,' loss = ', l)\n",
    "for i,j in model.named_parameters():\n",
    "    print(i,j)\n",
    "print()\n",
    "print()\n",
    "print(f\"Prediction after training: {model(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f05f80",
   "metadata": {},
   "source": [
    "learning rate=learning rate//2\n",
    "loss=nn.MSELoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),learning rate)\n",
    "for epoch in range(100):\n",
    "    y_predicted=model(X_test)\n",
    "    l=loss(y,y_predicted)\n",
    "    l.backword()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d590693",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
